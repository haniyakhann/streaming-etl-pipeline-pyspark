\# Streaming ETL Pipeline (PySpark + Python)



This project simulates a real-world data engineering workflow using a combination of PySpark, batch ETL, SQL modeling, and Airflow orchestration.  

It is designed to showcase strong DE fundamentals while remaining fully runnable on any machine.



---



\## ðŸ“Œ Features

\- Ingests raw JSON event data

\- Cleans and transforms data

\- Adds ingestion timestamps

\- Writes curated output to Parquet/CSV

\- Includes a PySpark streaming version (for production-style architecture)

\- Includes a lightweight local ETL version (runnable without Spark)

\- SQL star schema for analytics (fact + dimension tables)

\- Airflow DAG for orchestration



---



\## ðŸ“‚ Project Structure

streaming-etl-pipeline-pyspark/

â”‚

â”œâ”€â”€ src/

â”‚ â”œâ”€â”€ streaming\_etl.py # PySpark streaming pipeline (not run on Windows)

â”‚ â””â”€â”€ batch\_etl\_local.py # Local ETL pipeline (runs anywhere)

â”‚

â”œâ”€â”€ data/

â”‚ â””â”€â”€ raw/events1.json # Sample input data

â”‚

â”œâ”€â”€ output/

â”‚ â””â”€â”€ curated/ # ETL output (CSV/Parquet)

â”‚

â”œâ”€â”€ sql/

â”‚ â””â”€â”€ create\_star\_schema.sql # Warehouse modeling

â”‚

â”œâ”€â”€ dags/

â”‚ â””â”€â”€ streaming\_etl\_dag.py # Airflow DAG

â”‚

â””â”€â”€ README.md







---



\## ðŸš€ How to Run Locally



\### 1. Activate virtual environment  

.venv\\Scripts\\activate



\### 2. Run the ETL pipeline  

python src/batch\_etl\_local.py





Output will appear in `output/curated/events\_curated.csv`.



---



\## ðŸ§  Why Two ETL Versions?



\### PySpark Version

\- Shows distributed processing concepts

\- Reflects production-style engineering

\- Demonstrates Spark ETL skills



\### Local Python Version

\- Runs on any machine, no Hadoop required

\- Used for testing and demonstration



---



\## ðŸ“Š Star Schema



\- \*\*dim\_users\*\*

\- \*\*fact\_events\*\*



Enables analytics such as:

\- user activity breakdown

\- event type analysis

\- spend patterns

\- time-series ingestion insights



---



\## ðŸ›  Technologies Used

\- Python

\- PySpark (code only)

\- Airflow (DAG structure)

\- SQL (BigQuery-style schema)



---



\## ðŸ’¡ Author

\*\*Haniya A. Khan\*\*  

Data Engineering \& Analytics Enthusiast



